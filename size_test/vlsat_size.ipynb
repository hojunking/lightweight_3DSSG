{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genericpath import isfile\n",
    "import json\n",
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    current_dir = os.path.dirname(os.path.abspath(\"/home/knuvi/Desktop/song/lightweight_3DSSG/size_test\"))\n",
    "    os.sys.path.append(current_dir)\n",
    "    os.sys.path.append(os.path.join(current_dir, './src'))\n",
    "    #os.sys.path.append('./pytorch_geometric/torch_geometric')\n",
    "   # os.sys.path.append('./src')\n",
    "from src.model.model import MMGNet\n",
    "from src.utils.config import Config\n",
    "from utils import util\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name : Mmgnet\n",
      "num of data: 3845\n",
      "num of data: 548\n",
      "path: x/obj_encoder_best.pth result: False\n",
      "path: x/rel_encoder_2d_best.pth result: False\n",
      "path: x/rel_encoder_3d_best.pth result: False\n",
      "path: x/mmg_best.pth result: False\n",
      "path: x/triplet_projector_2d_best.pth result: False\n",
      "path: x/clip_adapter_best.pth result: False\n",
      "path: x/mlp_3d_best.pth result: False\n",
      "path: x/rel_predictor_3d_best.pth result: False\n",
      "path: x/rel_predictor_2d_best.pth result: False\n",
      "path: x/clip_model_best.pth result: False\n",
      "path: x/obj_predictor_2d_best.pth result: False\n",
      "path: x/obj_predictor_3d_best.pth result: False\n",
      "\tmodel loading failed!\n",
      "\n",
      "load pretrain model: x\n"
     ]
    }
   ],
   "source": [
    "def load_config():\n",
    "    # load config file\n",
    "    config = Config(\"../config/mmgnet.json\")\n",
    "    #print(config)\n",
    "    config.MODE = 'train'\n",
    "    config.exp = 'test'\n",
    "    #config.pruning.st_pruning_ratio = 0.5\n",
    "    #config.pruning_method = \"st\"\n",
    "    \n",
    "    return config\n",
    "config = load_config()\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "util.set_random_seed(config.SEED)\n",
    "config.MODEL.use_pretrain ='x'\n",
    "# if config.VERBOSE:\n",
    "#     print(config)\n",
    "model = MMGNet(config)\n",
    "#model.gcn_pruning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-1): 2 x GraphEdgeAttenNetwork(\n",
       "    (index_get): Gen_Index()\n",
       "    (index_aggr): Aggre_Index()\n",
       "    (edgeatten): MultiHeadedEdgeAttention(\n",
       "      (nn_edge): Sequential(\n",
       "        (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "      (nn): mySequential(\n",
       "        (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (proj_edge): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (proj_query): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (proj_value): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (prop): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.mmg.gcn_3ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "def get_submodule_parameters(model):\n",
    "    submodule_params = {}\n",
    "    for name, module in model.named_children():\n",
    "        submodule_params[name] = count_parameters(module)\n",
    "    return submodule_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 서브모듈의 파라미터 수:\n",
      "self_attn: 2,103,296\n",
      "cross_attn: 2,103,296\n",
      "cross_attn_rel: 2,103,296\n",
      "gcn_2ds: 7,520,576\n",
      "gcn_3ds: 7,520,576\n",
      "self_attn_fc: 1,608\n",
      "drop_out: 0\n",
      "총 파라미터 수: 21,352,648\n"
     ]
    }
   ],
   "source": [
    "submodule_params = get_submodule_parameters(model.model)\n",
    "\n",
    "print(\"각 서브모듈의 파라미터 수:\")\n",
    "for name, params in submodule_params.items():\n",
    "    print(f\"{name}: {params:,}\")\n",
    "# 전체 파라미터 수 계산 및 출력\n",
    "total_params = count_parameters(model.model)\n",
    "print(f\"총 파라미터 수: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(96, 32, kernel_size=(1,), stride=(1,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.gcn.gconvs[0].edgeatten.nn[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_params: 10677128\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMG(\n",
       "  (self_attn): ModuleList(\n",
       "    (0): MultiHeadAttention(\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): MultiHeadAttention(\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (cross_attn): ModuleList(\n",
       "    (0): MultiHeadAttention(\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): MultiHeadAttention(\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (cross_attn_rel): ModuleList(\n",
       "    (0): MultiHeadAttention(\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): MultiHeadAttention(\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (gcn_2ds): ModuleList(\n",
       "    (0): GraphEdgeAttenNetwork(\n",
       "      (index_get): Gen_Index()\n",
       "      (index_aggr): Aggre_Index()\n",
       "      (edgeatten): MultiHeadedEdgeAttention(\n",
       "        (nn_edge): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        )\n",
       "        (nn): mySequential(\n",
       "          (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "          (3): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (proj_edge): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (proj_query): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (proj_value): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (prop): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): GraphEdgeAttenNetwork(\n",
       "      (index_get): Gen_Index()\n",
       "      (index_aggr): Aggre_Index()\n",
       "      (edgeatten): MultiHeadedEdgeAttention(\n",
       "        (nn_edge): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        )\n",
       "        (nn): mySequential(\n",
       "          (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "          (3): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (proj_edge): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (proj_query): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (proj_value): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (prop): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (gcn_3ds): ModuleList(\n",
       "    (0): GraphEdgeAttenNetwork(\n",
       "      (index_get): Gen_Index()\n",
       "      (index_aggr): Aggre_Index()\n",
       "      (edgeatten): MultiHeadedEdgeAttention(\n",
       "        (nn_edge): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        )\n",
       "        (nn): mySequential(\n",
       "          (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "          (3): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (proj_edge): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (proj_query): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (proj_value): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (prop): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): GraphEdgeAttenNetwork(\n",
       "      (index_get): Gen_Index()\n",
       "      (index_aggr): Aggre_Index()\n",
       "      (edgeatten): MultiHeadedEdgeAttention(\n",
       "        (nn_edge): Sequential(\n",
       "          (0): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        )\n",
       "        (nn): mySequential(\n",
       "          (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "          (3): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (proj_edge): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (proj_query): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (proj_value): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (prop): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (self_attn_fc): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    (6): Linear(in_features=32, out_features=8, bias=True)\n",
       "  )\n",
       "  (drop_out): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.model.mmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gnn pruning start\n",
      "gnn: mmg pruning:0.5 start!\n",
      "gnn pruning success!\n"
     ]
    }
   ],
   "source": [
    "model.apply_pruning(\"gnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        num_params = param.numel()\n",
    "        if name.endswith('weight'):\n",
    "            non_zero_params = torch.count_nonzero(param).item()\n",
    "        else:\n",
    "            non_zero_params = num_params\n",
    "        print(f'name: {name}, num_params: {num_params}, non_zero_params: {non_zero_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', TripletGCN())\n",
      "('aggr_module', SumAggregation())\n",
      "('nn1', Sequential(\n",
      "  (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=1280, bias=True)\n",
      "  (3): ReLU()\n",
      "))\n",
      "('nn1.0', Linear(in_features=1280, out_features=512, bias=True))\n",
      "('nn1.1', ReLU())\n",
      "('nn1.2', Linear(in_features=512, out_features=1280, bias=True))\n",
      "('nn1.3', ReLU())\n",
      "('nn2', Sequential(\n",
      "  (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "))\n",
      "('nn2.0', Linear(in_features=512, out_features=512, bias=True))\n",
      "('nn2.1', ReLU())\n",
      "('nn2.2', Linear(in_features=512, out_features=512, bias=True))\n"
     ]
    }
   ],
   "source": [
    "tripletModel = model.model.gcn.gconvs[0]\n",
    "layers = []\n",
    "for m in tripletModel.named_modules():\n",
    "    print(m)\n",
    "#     if isinstance(m, torch.nn.Linear):\n",
    "#         layers.append(m)\n",
    "#     elif isinstance(m, torch.nn.modules.conv._ConvNd):\n",
    "#         layers.append(m)\n",
    "# print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TripletGCNModel(\n",
       "  (gconvs): ModuleList(\n",
       "    (0): TripletGCN()\n",
       "    (1): TripletGCN()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in model.model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.5)\n",
    "        prune.remove(module, 'weight')\n",
    "    elif isinstance(module, torch.nn.Conv1d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.5)\n",
    "        prune.remove(module, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SGFN' object has no attribute 'mmg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmmg\u001b[49m\u001b[38;5;241m.\u001b[39mself_attn\n",
      "File \u001b[0;32m~/miniconda3/envs/vlsat/lib/python3.8/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SGFN' object has no attribute 'mmg'"
     ]
    }
   ],
   "source": [
    "model.model.mmg.self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=512, bias=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.mmg.self_attn[0].attention.fc_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_pruning as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head #0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MMG_student' object has no attribute 'head_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmmg\u001b[38;5;241m.\u001b[39mmodules():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHead #\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39mhead_id)\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Before Pruning] Num Heads: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Head Dim: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m =>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m%\u001b[39m(m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      6\u001b[0m     head_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlsat/lib/python3.8/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MMG_student' object has no attribute 'head_dim'"
     ]
    }
   ],
   "source": [
    "def apply_pruning(self, apply_part):\n",
    "        if apply_part == \"encoder\":\n",
    "            encoders = ['obj_encoder', 'rel_encoder_2d', 'rel_encoder_3d']\n",
    "            for encoder_name in encoders:\n",
    "                print(f\"encoder: {encoder_name} pruning:{self.pruning_ratio} start!\")\n",
    "                for name, module in getattr(self.model, encoder_name).named_modules():\n",
    "                    if isinstance(module, torch.nn.Conv1d):\n",
    "                        prune.l1_unstructured(module, name='weight', amount=self.pruning_ratio)\n",
    "                        prune.remove(module, 'weight')\n",
    "        elif apply_part == \"gnn\":\n",
    "            print(\"gnn pruning start\")\n",
    "            gnn_name = 'mmg'\n",
    "            print(f\"gnn: {gnn_name} pruning:{self.pruning_ratio} start!\")\n",
    "            for name, module in getattr(self.model, gnn_name).named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=self.pruning_ratio)\n",
    "                    prune.remove(module, 'weight')\n",
    "        elif apply_part == \"classifier\":\n",
    "            print(f\"classifier: {apply_part} pruning:{self.pruning_ratio} start!\")\n",
    "            classifiers = ['obj_predictor_3d', 'rel_predictor_3d', 'obj_predictor_2d', 'rel_predictor_2d']\n",
    "            for predicator in classifiers:\n",
    "                for name, module in getattr(self.model, predicator).named_modules():\n",
    "                    if isinstance(module, torch.nn.Linear):\n",
    "                        prune.l1_unstructured(module, name='weight', amount=self.pruning_ratio)\n",
    "                        prune.remove(module, 'weight')\n",
    "        elif apply_part == 'all':\n",
    "            print(f\"ALL Pruning start!\")\n",
    "            for name, module in self.model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=self.pruning_ratio)\n",
    "                    prune.remove(module, 'weight')\n",
    "                elif isinstance(module, torch.nn.Conv1d):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=self.pruning_ratio)\n",
    "                    prune.remove(module, 'weight')\n",
    "        else:\n",
    "            print(\"pruning error!\")\n",
    "        print(f\"{apply_part} pruning success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.862275242805481\n",
      "Epoch 2, Loss: 0.6179301738739014\n",
      "Epoch 3, Loss: 0.45785441994667053\n",
      "Epoch 4, Loss: 0.3521365225315094\n",
      "Epoch 5, Loss: 0.28182896971702576\n",
      "conv1 weight after pruning and training:\n",
      "Parameter containing:\n",
      "tensor([[[ 4.7807e-03, -4.5514e-01,  6.1754e-04]],\n",
      "\n",
      "        [[-4.4016e-01, -9.2592e-04, -2.4921e-01]],\n",
      "\n",
      "        [[ 1.5271e-03,  3.5766e-01, -2.8726e-01]],\n",
      "\n",
      "        [[ 1.4692e-01,  4.4684e-03,  3.4911e-01]],\n",
      "\n",
      "        [[ 3.2163e-01,  4.3871e-04,  4.4892e-01]],\n",
      "\n",
      "        [[-4.4697e-03,  2.4309e-01, -5.4016e-01]],\n",
      "\n",
      "        [[-4.4790e-01,  4.4176e-01, -5.0021e-01]],\n",
      "\n",
      "        [[-1.7102e-03,  3.6067e-01, -1.3293e-01]],\n",
      "\n",
      "        [[-2.9553e-03, -3.6057e-03, -5.1928e-01]],\n",
      "\n",
      "        [[ 2.9437e-01, -2.0267e-01, -1.5559e-03]],\n",
      "\n",
      "        [[ 2.3055e-01,  5.6920e-01, -2.5067e-03]],\n",
      "\n",
      "        [[-4.3774e-01,  2.5613e-01, -4.2329e-01]],\n",
      "\n",
      "        [[ 3.3195e-01,  9.8718e-04,  4.0817e-01]],\n",
      "\n",
      "        [[-7.6961e-04, -5.4603e-01,  4.5536e-01]],\n",
      "\n",
      "        [[-3.9488e-01,  4.3384e-01,  3.0693e-01]],\n",
      "\n",
      "        [[-4.3849e-01,  3.1398e-01,  4.9767e-01]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.optim as optim\n",
    "\n",
    "# Seed 설정\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_random_seed(2020)\n",
    "\n",
    "# 간단한 모델 정의\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 3, 1)  # Conv1d 출력 크기에 맞게 Linear 층 수정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Pruning 적용\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv1d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.3)\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "# 학습 설정\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy 데이터\n",
    "inputs = torch.randn(10, 1, 5)\n",
    "targets = torch.randn(10, 1)\n",
    "\n",
    "# 학습\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# 가지치기된 파라미터 확인\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv1d):\n",
    "        print(f'{name} weight after pruning and training:')\n",
    "        print(module.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
