{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_loss(baseline_acc, current_acc):\n",
    "    \"\"\"Calculate normalized performance loss.\"\"\"\n",
    "    return (baseline_acc - current_acc) / baseline_acc if baseline_acc != 0 else 0\n",
    "\n",
    "def calculate_differential_losses(pruning_results, baseline):\n",
    "    \"\"\"\n",
    "    Calculate the differential of performance losses across pruning ratios.\n",
    "    Arguments:\n",
    "    pruning_results -- list of tuples, where each tuple contains (pruning_ratio, obj_acc, rel_acc, triplet_acc)\n",
    "    baseline -- tuple of (baseline_obj_acc, baseline_rel_acc, baseline_triplet_acc)\n",
    "    \"\"\"\n",
    "    # Sorting by pruning ratios for accurate differential calculation\n",
    "    sorted_results = sorted(pruning_results, key=lambda x: x[0])\n",
    "    ratios = [x[0] for x in sorted_results]\n",
    "    losses = [[calculate_performance_loss(x[i + 1], baseline[i]) for i in range(3)] for x in sorted_results]\n",
    "    \n",
    "    # Calculating differentials of losses\n",
    "    diffs = np.diff(np.array(losses), axis=0)\n",
    "    diffs = np.vstack([diffs, diffs[-1]])  # Duplicate last differential as an approximation for the last point\n",
    "    return ratios, losses, diffs\n",
    "\n",
    "def calculate_model_score(losses, diffs, weights=(1, 1, 5), mu=5, pruning_ratio=0, detailed=False):\n",
    "    \"\"\"Calculate scores incorporating the absolute values of losses and their differentials, adjusted by lambda.\"\"\"\n",
    "    adjusted_losses = np.abs(losses)\n",
    "    adjusted_diffs = np.abs(diffs)\n",
    "    \n",
    "    weighted_diff_loss = sum(weights[i] * (adjusted_diffs[i] - adjusted_losses[i]) for i in range(3))\n",
    "    score = weighted_diff_loss + mu * pruning_ratio\n",
    "    \n",
    "    if detailed:\n",
    "        return score, weighted_diff_loss\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sgpn_pruning_results = [\n",
    "    (0.75, 29.989, 79.029, 76.868),\n",
    "    (0.55, 22.592, 77.589, 75.373),\n",
    "    (0.6, 24.510, 78.692, 76.251),\n",
    "    (0.3, 42.276, 81.235, 83.209),\n",
    "    (0.7, 30.074, 78.650, 76.759),\n",
    "    (0.35, 38.377, 81.069, 81.855),\n",
    "    (0.2, 46.491, 82.212, 84.951),\n",
    "    (0.25, 45.817, 81.486, 84.443),\n",
    "    (0.1, 48.957, 82.641, 85.769),\n",
    "    (0.4, 30.285, 78.940, 79.357),\n",
    "    (0.15, 47.524, 82.594, 85.095),\n",
    "    (0.05, 49.989, 82.500, 85.943),\n",
    "    (0.65, 31.191, 79.042, 78.219),\n",
    "    (0.5, 33.867, 77.175, 77.054),\n",
    "    (0.45, 33.783, 78.675, 79.545)\n",
    "]\n",
    "# SGPN 모델의 Baseline 성능\n",
    "sgpn_baseline = (50.158, 82.324, 85.980)\n",
    "\n",
    "# SGFN 모델의 unstructured pruning 결과\n",
    "sgfn_pruning_results = [\n",
    "   (0.65, 49.631, 89.071, 87.256),\n",
    "    (0.75, 43.267, 87.398, 84.398),\n",
    "    (0.45, 51.949, 89.807, 88.417),\n",
    "    (0.5, 51.823, 89.681, 88.208),\n",
    "    (0.35, 52.055, 89.879, 88.528),\n",
    "    (0.3, 52.308, 89.998, 88.506),\n",
    "    (0.25, 52.624, 89.986, 88.491),\n",
    "    (0.6, 50.896, 89.267, 87.631),\n",
    "    (0.05, 52.476, 89.956, 88.508),\n",
    "    (0.7, 46.575, 88.337, 85.920),\n",
    "    (0.1, 52.476, 89.946, 88.516),\n",
    "    (0.4, 51.970, 89.874, 88.508),\n",
    "    (0.15, 52.497, 89.973, 88.508),\n",
    "    (0.2, 52.518, 89.906, 88.513),\n",
    "    (0.55, 51.149, 89.525, 87.928)\n",
    "]\n",
    "\n",
    "# SGFN 모델의 Baseline 성능\n",
    "sgfn_baseline = (52.476, 89.968, 88.486)\n",
    "\n",
    "# Attn + SGFN 모델의 unstructured pruning 결과\n",
    "attn_sgfn_pruning_results = [\n",
    "    (0.2, 54.499, 88.620, 89.066),\n",
    "    (0.45, 51.886, 88.322, 88.030),\n",
    "    (0.6, 45.838, 88.412, 85.464),\n",
    "    (0.55, 49.463, 88.196, 87.147),\n",
    "    (0.5, 50.664, 88.186, 87.576),\n",
    "    (0.7, 43.077, 87.576, 84.755),\n",
    "    (0.15, 54.626, 88.632, 89.106),\n",
    "    (0.4, 52.750, 88.498, 88.417),\n",
    "    (0.35, 53.678, 88.707, 88.655),\n",
    "    (0.75, 36.164, 86.476, 81.444),\n",
    "    (0.3, 53.656, 88.548, 88.689),\n",
    "    (0.05, 54.478, 88.764, 89.187),\n",
    "    (0.25, 54.542, 88.751, 88.979),\n",
    "    (0.1, 54.457, 88.677, 89.195),\n",
    "    (0.65, 43.498, 87.435, 84.921)\n",
    "]\n",
    "\n",
    "# Attn + SGFN 모델의 Baseline 성능\n",
    "attn_sgfn_baseline = (54.499, 88.734, 89.180)\n",
    "\n",
    "# VLSAT 모델의 unstructured pruning 결과\n",
    "vlsat_pruning_results = [\n",
    "    (0.5, 54.394, 88.565, 88.843),\n",
    "    (0.65, 51.570, 88.959, 87.879),\n",
    "    (0.4, 54.689, 88.776, 89.123),\n",
    "    (0.3, 54.689, 89.078, 89.215),\n",
    "    (0.15, 54.773, 88.778, 89.247),\n",
    "    (0.35, 54.605, 89.006, 89.277),\n",
    "    (0.75, 46.175, 87.631, 84.931),\n",
    "    (0.25, 54.689, 88.746, 89.259),\n",
    "    (0.1, 54.773, 88.788, 89.314),\n",
    "    (0.55, 53.846, 88.739, 88.471),\n",
    "    (0.45, 54.858, 88.816, 88.984),\n",
    "    (0.05, 54.731, 88.751, 89.344),\n",
    "    (0.2, 55.047, 88.761, 89.304),\n",
    "    (0.7, 47.334, 88.094, 85.801),\n",
    "    (0.6, 53.256, 88.756, 88.236)\n",
    "]\n",
    "\n",
    "# VLSAT 모델의 Baseline 성능\n",
    "vlsat_baseline = (54.837, 88.766, 89.336)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_data = {\n",
    "    \"SGPN\": {\n",
    "        \"baseline\": sgpn_baseline,\n",
    "        \"pruning_results\": sgpn_pruning_results\n",
    "    },\n",
    "    \"SGFN\": {\n",
    "        \"baseline\": sgfn_baseline,\n",
    "        \"pruning_results\": sgfn_pruning_results\n",
    "    },\n",
    "    \"Attn + SGFN\": {\n",
    "        \"baseline\": attn_sgfn_baseline,\n",
    "        \"pruning_results\": attn_sgfn_pruning_results\n",
    "    },\n",
    "    \"VLSAT\": {\n",
    "        \"baseline\": vlsat_baseline,\n",
    "        \"pruning_results\": vlsat_pruning_results\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(models_data, weights=(1, 1, 3), mu=0.5):\n",
    "    results = {}\n",
    "    for model_name, data in models_data.items():\n",
    "        baseline = data[\"baseline\"]\n",
    "        pruning_results = data[\"pruning_results\"]\n",
    "        if not pruning_results or len(pruning_results) < 2:\n",
    "            print(f\"Not enough data to calculate scores for {model_name}.\")\n",
    "            continue\n",
    "        \n",
    "        ratios, losses, diffs = calculate_differential_losses(pruning_results, baseline)\n",
    "        \n",
    "        # Store detailed score components for each pruning ratio\n",
    "        detailed_scores = []\n",
    "        scores = []\n",
    "        for idx in range(len(ratios)):\n",
    "            score, components = calculate_model_score(losses[idx], diffs[idx], weights, mu, ratios[idx], detailed=True)\n",
    "            scores.append(score)\n",
    "            detailed_scores.append({\n",
    "                \"ratio\": ratios[idx],\n",
    "                \"weighted_diff_loss\": components,\n",
    "                \"weighted_ratio\": mu * ratios[idx],\n",
    "                \"total_score\": score\n",
    "            })\n",
    "        \n",
    "        max_score_index = np.argmax(scores)\n",
    "        best_ratio = ratios[max_score_index]\n",
    "        best_score = scores[max_score_index]\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"best_ratio\": best_ratio,\n",
    "            \"best_score\": best_score,\n",
    "            \"all_scores\": scores,\n",
    "            \"detailed_scores\": detailed_scores\n",
    "        }\n",
    "\n",
    "        print(f\"Scores for {model_name}:\")\n",
    "        for entry in detailed_scores:\n",
    "            print(f\"  Pruning Ratio {entry['ratio']}, Weighted Diff-Loss: {entry['weighted_diff_loss']}, \"\n",
    "                  f\"Weighted Ratio: {entry['weighted_ratio']:.3f}, Total Score: {entry['total_score']:.3f}\")\n",
    "        print(f\"Best Pruning Ratio: {best_ratio}, Highest Score: {best_score:.3f}\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for SGPN:\n",
      "  Pruning Ratio 0.05, Weighted Diff-Loss: 0.022136627635894704, Weighted Ratio: 0.025, Total Score: 0.047\n",
      "  Pruning Ratio 0.1, Weighted Diff-Loss: 0.01953200633501183, Weighted Ratio: 0.050, Total Score: 0.070\n",
      "  Pruning Ratio 0.15, Weighted Diff-Loss: -0.05667369322714977, Weighted Ratio: 0.075, Total Score: 0.018\n",
      "  Pruning Ratio 0.2, Weighted Diff-Loss: -0.07351746796082102, Weighted Ratio: 0.100, Total Score: 0.026\n",
      "  Pruning Ratio 0.25, Weighted Diff-Loss: -0.01951860613461024, Weighted Ratio: 0.125, Total Score: 0.105\n",
      "  Pruning Ratio 0.3, Weighted Diff-Loss: -0.12586082814365815, Weighted Ratio: 0.150, Total Score: 0.024\n",
      "  Pruning Ratio 0.35, Weighted Diff-Loss: 0.002155624367787705, Weighted Ratio: 0.175, Total Score: 0.177\n",
      "  Pruning Ratio 0.4, Weighted Diff-Loss: -0.7667593827025527, Weighted Ratio: 0.200, Total Score: -0.567\n",
      "  Pruning Ratio 0.45, Weighted Diff-Loss: -0.6449347072334441, Weighted Ratio: 0.225, Total Score: -0.420\n",
      "  Pruning Ratio 0.5, Weighted Diff-Loss: -0.0757823983896502, Weighted Ratio: 0.250, Total Score: 0.174\n",
      "  Pruning Ratio 0.55, Weighted Diff-Loss: -1.4753599645951754, Weighted Ratio: 0.275, Total Score: -1.200\n",
      "  Pruning Ratio 0.6, Weighted Diff-Loss: -0.9472785569054634, Weighted Ratio: 0.300, Total Score: -0.647\n",
      "  Pruning Ratio 0.65, Weighted Diff-Loss: -0.8196367412531886, Weighted Ratio: 0.325, Total Score: -0.495\n",
      "  Pruning Ratio 0.7, Weighted Diff-Loss: -1.060408338641098, Weighted Ratio: 0.350, Total Score: -0.710\n",
      "  Pruning Ratio 0.75, Weighted Diff-Loss: -1.0553507478338355, Weighted Ratio: 0.375, Total Score: -0.680\n",
      "Best Pruning Ratio: 0.35, Highest Score: 0.177\n",
      "\n",
      "Scores for SGFN:\n",
      "  Pruning Ratio 0.05, Weighted Diff-Loss: -0.0004968311798370429, Weighted Ratio: 0.025, Total Score: 0.025\n",
      "  Pruning Ratio 0.1, Weighted Diff-Loss: -0.00029010021870623076, Weighted Ratio: 0.050, Total Score: 0.050\n",
      "  Pruning Ratio 0.15, Weighted Diff-Loss: 0.00011301844287874973, Weighted Ratio: 0.075, Total Score: 0.075\n",
      "  Pruning Ratio 0.2, Weighted Diff-Loss: 0.0012434757220778862, Weighted Ratio: 0.100, Total Score: 0.101\n",
      "  Pruning Ratio 0.25, Weighted Diff-Loss: 0.0034839269360639248, Weighted Ratio: 0.125, Total Score: 0.128\n",
      "  Pruning Ratio 0.3, Weighted Diff-Loss: 0.0027217664413465292, Weighted Ratio: 0.150, Total Score: 0.153\n",
      "  Pruning Ratio 0.35, Weighted Diff-Loss: -0.008119039889229582, Weighted Ratio: 0.175, Total Score: 0.167\n",
      "  Pruning Ratio 0.4, Weighted Diff-Loss: -0.007286113286652241, Weighted Ratio: 0.200, Total Score: 0.193\n",
      "  Pruning Ratio 0.45, Weighted Diff-Loss: -0.0033012143909116973, Weighted Ratio: 0.225, Total Score: 0.222\n",
      "  Pruning Ratio 0.5, Weighted Diff-Loss: -0.0005810250806811941, Weighted Ratio: 0.250, Total Score: 0.249\n",
      "  Pruning Ratio 0.55, Weighted Diff-Loss: -0.03169390092036568, Weighted Ratio: 0.275, Total Score: 0.243\n",
      "  Pruning Ratio 0.6, Weighted Diff-Loss: -0.026650995764996292, Weighted Ratio: 0.300, Total Score: 0.273\n",
      "  Pruning Ratio 0.65, Weighted Diff-Loss: 0.015391194698152326, Weighted Ratio: 0.325, Total Score: 0.340\n",
      "  Pruning Ratio 0.7, Weighted Diff-Loss: -0.0819560559180121, Weighted Ratio: 0.350, Total Score: 0.268\n",
      "  Pruning Ratio 0.75, Weighted Diff-Loss: -0.2347572293916234, Weighted Ratio: 0.375, Total Score: 0.140\n",
      "Best Pruning Ratio: 0.65, Highest Score: 0.340\n",
      "\n",
      "Scores for Attn + SGFN:\n",
      "  Pruning Ratio 0.05, Weighted Diff-Loss: 0.0006766714662454938, Weighted Ratio: 0.025, Total Score: 0.026\n",
      "  Pruning Ratio 0.1, Weighted Diff-Loss: 0.0046815763727285145, Weighted Ratio: 0.050, Total Score: 0.055\n",
      "  Pruning Ratio 0.15, Weighted Diff-Loss: -0.0021582407782697287, Weighted Ratio: 0.075, Total Score: 0.073\n",
      "  Pruning Ratio 0.2, Weighted Diff-Loss: 7.711213839956053e-05, Weighted Ratio: 0.100, Total Score: 0.100\n",
      "  Pruning Ratio 0.25, Weighted Diff-Loss: 0.02086659159758646, Weighted Ratio: 0.125, Total Score: 0.146\n",
      "  Pruning Ratio 0.3, Weighted Diff-Loss: -0.031050981200774694, Weighted Ratio: 0.150, Total Score: 0.119\n",
      "  Pruning Ratio 0.35, Weighted Diff-Loss: -0.005017738296233851, Weighted Ratio: 0.175, Total Score: 0.170\n",
      "  Pruning Ratio 0.4, Weighted Diff-Loss: -0.029207283434215463, Weighted Ratio: 0.200, Total Score: 0.171\n",
      "  Pruning Ratio 0.45, Weighted Diff-Loss: -0.0515772071252776, Weighted Ratio: 0.225, Total Score: 0.173\n",
      "  Pruning Ratio 0.5, Weighted Diff-Loss: -0.09558404394503234, Weighted Ratio: 0.250, Total Score: 0.154\n",
      "  Pruning Ratio 0.55, Weighted Diff-Loss: -0.02785046609946735, Weighted Ratio: 0.275, Total Score: 0.247\n",
      "  Pruning Ratio 0.6, Weighted Diff-Loss: -0.2278395082004696, Weighted Ratio: 0.300, Total Score: 0.072\n",
      "  Pruning Ratio 0.65, Weighted Diff-Loss: -0.39817310267249445, Weighted Ratio: 0.325, Total Score: -0.073\n",
      "  Pruning Ratio 0.7, Weighted Diff-Loss: -0.0519439194505201, Weighted Ratio: 0.350, Total Score: 0.298\n",
      "  Pruning Ratio 0.75, Weighted Diff-Loss: -0.435003826264127, Weighted Ratio: 0.375, Total Score: -0.060\n",
      "Best Pruning Ratio: 0.7, Highest Score: 0.298\n",
      "\n",
      "Scores for VLSAT:\n",
      "  Pruning Ratio 0.05, Weighted Diff-Loss: -0.00018171178493544258, Weighted Ratio: 0.025, Total Score: 0.025\n",
      "  Pruning Ratio 0.1, Weighted Diff-Loss: 0.00021013787360211296, Weighted Ratio: 0.050, Total Score: 0.050\n",
      "  Pruning Ratio 0.15, Weighted Diff-Loss: 0.0027962729962682665, Weighted Ratio: 0.075, Total Score: 0.078\n",
      "  Pruning Ratio 0.2, Weighted Diff-Loss: 0.0032569262976951284, Weighted Ratio: 0.100, Total Score: 0.103\n",
      "  Pruning Ratio 0.25, Weighted Diff-Loss: -0.0003107892947478686, Weighted Ratio: 0.125, Total Score: 0.125\n",
      "  Pruning Ratio 0.3, Weighted Diff-Loss: -0.005842768730742593, Weighted Ratio: 0.150, Total Score: 0.144\n",
      "  Pruning Ratio 0.35, Weighted Diff-Loss: 0.0003858248187585474, Weighted Ratio: 0.175, Total Score: 0.175\n",
      "  Pruning Ratio 0.4, Weighted Diff-Loss: -0.0017519487082309727, Weighted Ratio: 0.200, Total Score: 0.198\n",
      "  Pruning Ratio 0.45, Weighted Diff-Loss: 0.0033265403987801133, Weighted Ratio: 0.225, Total Score: 0.228\n",
      "  Pruning Ratio 0.5, Weighted Diff-Loss: -0.002151528988803996, Weighted Ratio: 0.250, Total Score: 0.248\n",
      "  Pruning Ratio 0.55, Weighted Diff-Loss: -0.028498139175007868, Weighted Ratio: 0.275, Total Score: 0.247\n",
      "  Pruning Ratio 0.6, Weighted Diff-Loss: -0.018913823738200053, Weighted Ratio: 0.300, Total Score: 0.281\n",
      "  Pruning Ratio 0.65, Weighted Diff-Loss: 0.06356076370710635, Weighted Ratio: 0.325, Total Score: 0.389\n",
      "  Pruning Ratio 0.7, Weighted Diff-Loss: -0.2233404291807622, Weighted Ratio: 0.350, Total Score: 0.127\n",
      "  Pruning Ratio 0.75, Weighted Diff-Loss: -0.28974002530365284, Weighted Ratio: 0.375, Total Score: 0.085\n",
      "Best Pruning Ratio: 0.65, Highest Score: 0.389\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_results = display_scores(models_data)\n",
    "#final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
