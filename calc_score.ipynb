{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_loss(baseline_acc, current_acc):\n",
    "    \"\"\"Calculate normalized performance loss.\"\"\"\n",
    "    #print('check: baseline_acc: ', baseline_acc, 'current_acc: ', current_acc)\n",
    "    return (current_acc - baseline_acc) / baseline_acc if baseline_acc != 0 else 0\n",
    "\n",
    "def calculate_differential_losses(pruning_results, baseline):\n",
    "    \"\"\"\n",
    "    Calculate the differential of performance losses across pruning ratios.\n",
    "    Arguments:\n",
    "    pruning_results -- list of tuples, where each tuple contains (pruning_ratio, obj_acc, rel_acc, triplet_acc)\n",
    "    baseline -- tuple of (baseline_obj_acc, baseline_rel_acc, baseline_triplet_acc)\n",
    "    \"\"\"\n",
    "    # Sorting by pruning ratios for accurate differential calculation\n",
    "    sorted_results = sorted(pruning_results, key=lambda x: x[0])\n",
    "    ratios = [x[0] for x in sorted_results]\n",
    "    losses = [[calculate_performance_loss(baseline[i], x[i + 1]) for i in range(3)] for x in sorted_results]\n",
    "    \n",
    "    # Calculating differentials of losses\n",
    "    diffs = -np.diff(np.array(losses), axis=0)\n",
    "    diffs = np.vstack([diffs, diffs[-1]])  # Duplicate last differential as an approximation for the last point\n",
    "    return ratios, losses, diffs\n",
    "\n",
    "def calculate_model_score(losses, diffs, weights=(1, 1, 5), mu=5, pruning_ratio=0, detailed=False):\n",
    "    \"\"\"Calculate scores incorporating the absolute values of losses and their differentials, adjusted by lambda.\"\"\"\n",
    "    adjusted_losses = losses\n",
    "    adjusted_diffs = diffs\n",
    "    #print(f'pruning ratio: {pruning_ratio}, adjusted losses: {adjusted_losses}, adjusted diffs: {adjusted_diffs}')\n",
    "    weighted_diff_loss = sum(weights[i] * (adjusted_diffs[i] + adjusted_losses[i]) for i in range(3))\n",
    "    score = weighted_diff_loss + mu * pruning_ratio\n",
    "    \n",
    "    if detailed:\n",
    "        return score, weighted_diff_loss\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGFN Pruning Results: [(0.0, 52.39, 92.14, 89.19), (0.05, 52.39, 92.14, 89.19), (0.1, 52.39, 92.07, 89.18), (0.15, 52.39, 91.79, 89.09), (0.2, 52.39, 90.99, 88.87), (0.25, 52.39, 90.39, 88.66), (0.3, 52.46, 89.63, 88.36), (0.35, 52.46, 88.57, 88.13), (0.4, 52.46, 89.17, 87.92), (0.45, 52.27, 85.12, 87.1), (0.5, 52.27, 83.41, 86.65), (0.55, 52.41, 85.73, 86.51), (0.6, 48.6, 67.92, 84.27), (0.65, 41.85, 67.92, 81.56), (0.7, 32.5, 67.92, 76.68), (0.75, 28.47, 67.92, 74.99)]\n",
      "SGFN Baseline: (52.39, 92.14, 89.19)\n",
      "Attention SGFN Pruning Results: [(0.0, 54.6259220231823, 90.18144861434732, 89.50473451985523), (0.05, 54.478398314014754, 88.98170641019286, 89.40062465916415), (0.1, 54.373024236037935, 85.12220514600169, 89.20727777502354), (0.15, 54.3940990516333, 81.01234445491052, 89.00897327846909), (0.2, 54.58377239199157, 73.1247831044569, 88.68672847156809), (0.25, 54.58377239199157, 70.74760795201031, 88.5256060681176), (0.3, 54.64699683877766, 74.58727876654604, 88.30994992811462), (0.35, 54.604847207586936, 74.50299935551038, 87.88359526052253), (0.4, 54.836670179135936, 76.17619354518864, 86.98130980119974), (0.45, 53.50895679662803, 72.93639383273016, 86.36656586188091), (0.5, 49.82086406743941, 73.10247384859451, 85.18913291358882), (0.55, 45.39515279241306, 73.86594616032919, 83.12180853700858), (0.6, 39.852476290832456, 74.26751276585196, 80.50171037628279), (0.65, 33.88830347734457, 71.44167368995092, 78.58063556591145), (0.7, 30.389884088514226, 73.96014079619255, 77.21977095830648), (0.75, 27.544783983140142, 76.79589509692133, 76.68434881760943)]\n",
      "Attention SGFN Baseline: (54.6259220231823, 90.18144861434732, 89.50473451985523)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_dir = '/home/song/Desktop/song/lightweight_3DSSG/visualization/pruning_infer_data/'\n",
    "# CSV 파일 경로\n",
    "file_path1 = base_dir + 'SGFN_param60_Structured_Pruning_Results.csv'\n",
    "file_path2 = base_dir +'Attn+SGFN_param60_Structured_Pruning_Results.csv'\n",
    "file_path3 = base_dir +'SGPN_param40_Structured_Pruning_results.csv'\n",
    "\n",
    "# CSV 파일 로드\n",
    "df1 = pd.read_csv(file_path1).sort_values(by='Pruning Ratio')\n",
    "df2 = pd.read_csv(file_path2).sort_values(by='Pruning Ratio')\n",
    "df3 = pd.read_csv(file_path3).sort_values(by='Pruning Ratio')\n",
    "\n",
    "# 필요한 데이터를 추출하여 리스트로 변환\n",
    "sgfn_pruning_results = [\n",
    "    (row['Pruning Ratio'], row['3d obj Acc@1'], row['3d rel Acc@1'], row['3d triplet Acc@50'])\n",
    "    for index, row in df1.iterrows()\n",
    "]\n",
    "sgfn_baseline = (\n",
    "    df1['3d obj Acc@1'].iloc[0],\n",
    "    df1['3d rel Acc@1'].iloc[0],\n",
    "    df1['3d triplet Acc@50'].iloc[0]\n",
    ")\n",
    "attn_sgfn_pruning_results = [\n",
    "    (row['Pruning Ratio'], row['3d obj Acc@1'], row['3d rel Acc@1'], row['3d triplet Acc@50'])\n",
    "    for index, row in df2.iterrows()\n",
    "]\n",
    "attn_sgfn_baseline = (\n",
    "    df2['3d obj Acc@1'].iloc[0],\n",
    "    df2['3d rel Acc@1'].iloc[0],\n",
    "    df2['3d triplet Acc@50'].iloc[0]\n",
    ")\n",
    "\n",
    "sgpn_pruning_results = [\n",
    "    (row['Pruning Ratio'], row['3d obj Acc@1'], row['3d rel Acc@1'], row['3d triplet Acc@50'])\n",
    "    for index, row in df2.iterrows()\n",
    "]\n",
    "sgpn_baseline = (\n",
    "    df2['3d obj Acc@1'].iloc[0],\n",
    "    df2['3d rel Acc@1'].iloc[0],\n",
    "    df2['3d triplet Acc@50'].iloc[0]\n",
    ")\n",
    "print(\"SGFN Pruning Results:\", sgfn_pruning_results)\n",
    "print(\"SGFN Baseline:\", sgfn_baseline)\n",
    "print(\"Attention SGFN Pruning Results:\", attn_sgfn_pruning_results)\n",
    "print(\"Attention SGFN Baseline:\", attn_sgfn_baseline)\n",
    "print(\"SGPN Pruning Results:\", sgpn_pruning_results)\n",
    "print(\"SGPN Baseline:\", sgpn_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_data = {\n",
    "    \"SGPN\": {\n",
    "        \"baseline\": sgpn_baseline,\n",
    "        \"pruning_results\": sgpn_pruning_results\n",
    "    },\n",
    "    \"SGFN\": {\n",
    "        \"baseline\": sgfn_baseline,\n",
    "        \"pruning_results\": sgfn_pruning_results\n",
    "    },\n",
    "    \"Attn + SGFN\": {\n",
    "        \"baseline\": attn_sgfn_baseline,\n",
    "        \"pruning_results\": attn_sgfn_pruning_results\n",
    "    },\n",
    "    # \"VLSAT\": {\n",
    "    #     \"baseline\": vlsat_baseline,\n",
    "    #     \"pruning_results\": vlsat_pruning_results\n",
    "    # }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(models_data, weights=(0, 0, 3), mu=0.1):\n",
    "    results = {}\n",
    "    for model_name, data in models_data.items():\n",
    "        baseline = data[\"baseline\"]\n",
    "        pruning_results = data[\"pruning_results\"]\n",
    "        if not pruning_results or len(pruning_results) < 2:\n",
    "            print(f\"Not enough data to calculate scores for {model_name}.\")\n",
    "            continue\n",
    "        \n",
    "        ratios, losses, diffs = calculate_differential_losses(pruning_results, baseline)\n",
    "        \n",
    "        # Store detailed score components for each pruning ratio\n",
    "        detailed_scores = []\n",
    "        scores = []\n",
    "        for idx in range(len(ratios)):\n",
    "            score, components = calculate_model_score(losses[idx], diffs[idx], weights, mu, ratios[idx], detailed=True)\n",
    "            scores.append(score)\n",
    "            detailed_scores.append({\n",
    "                \"ratio\": ratios[idx],\n",
    "                \"weighted_diff_loss\": components,\n",
    "                \"weighted_ratio\": mu * ratios[idx],\n",
    "                \"total_score\": score\n",
    "            })\n",
    "        \n",
    "        max_score_index = np.argmax(scores)\n",
    "        best_ratio = ratios[max_score_index]\n",
    "        best_score = scores[max_score_index]\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"best_ratio\": best_ratio,\n",
    "            \"best_score\": best_score,\n",
    "            \"all_scores\": scores,\n",
    "            \"detailed_scores\": detailed_scores\n",
    "        }\n",
    "\n",
    "        print(f\"Scores for {model_name}:\")\n",
    "        for entry in detailed_scores:\n",
    "            print(f\"  Pruning Ratio {entry['ratio']}, Weighted Diff-Loss: {entry['weighted_diff_loss']}, \"\n",
    "                  f\"Weighted Ratio: {entry['weighted_ratio']:.3f}, Total Score: {entry['total_score']:.3f}\")\n",
    "        print(f\"Best Pruning Ratio: {best_ratio}, Highest Score: {best_score:.3f}\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for SGFN:\n",
      "  Pruning Ratio 0.0, Weighted Diff-Loss: 0.0, Weighted Ratio: 0.000, Total Score: 0.000\n",
      "  Pruning Ratio 0.05, Weighted Diff-Loss: 0.0003363605785398892, Weighted Ratio: 0.005, Total Score: 0.005\n",
      "  Pruning Ratio 0.1, Weighted Diff-Loss: 0.002690884628321981, Weighted Ratio: 0.010, Total Score: 0.013\n",
      "  Pruning Ratio 0.15, Weighted Diff-Loss: 0.004036326942482495, Weighted Ratio: 0.015, Total Score: 0.019\n",
      "  Pruning Ratio 0.2, Weighted Diff-Loss: -0.0036999663639416484, Weighted Ratio: 0.020, Total Score: 0.016\n",
      "  Pruning Ratio 0.25, Weighted Diff-Loss: -0.007736293306424622, Weighted Ratio: 0.025, Total Score: 0.017\n",
      "  Pruning Ratio 0.3, Weighted Diff-Loss: -0.020181634712411516, Weighted Ratio: 0.030, Total Score: 0.010\n",
      "  Pruning Ratio 0.35, Weighted Diff-Loss: -0.028590649175916874, Weighted Ratio: 0.035, Total Score: 0.006\n",
      "  Pruning Ratio 0.4, Weighted Diff-Loss: -0.015136226034308402, Weighted Ratio: 0.040, Total Score: 0.025\n",
      "  Pruning Ratio 0.45, Weighted Diff-Loss: -0.05516313488059249, Weighted Ratio: 0.045, Total Score: -0.010\n",
      "  Pruning Ratio 0.5, Weighted Diff-Loss: -0.08072653884964653, Weighted Ratio: 0.050, Total Score: -0.031\n",
      "  Pruning Ratio 0.55, Weighted Diff-Loss: -0.01479986545576803, Weighted Ratio: 0.055, Total Score: 0.040\n",
      "  Pruning Ratio 0.6, Weighted Diff-Loss: -0.07433568785738338, Weighted Ratio: 0.060, Total Score: -0.014\n",
      "  Pruning Ratio 0.65, Weighted Diff-Loss: -0.09249915909855366, Weighted Ratio: 0.065, Total Score: -0.027\n",
      "  Pruning Ratio 0.7, Weighted Diff-Loss: -0.36394214598049046, Weighted Ratio: 0.070, Total Score: -0.294\n",
      "  Pruning Ratio 0.75, Weighted Diff-Loss: -0.4207870837537838, Weighted Ratio: 0.075, Total Score: -0.346\n",
      "Best Pruning Ratio: 0.55, Highest Score: 0.040\n",
      "\n",
      "Scores for Attn + SGFN:\n",
      "  Pruning Ratio 0.0, Weighted Diff-Loss: 0.003489531405782351, Weighted Ratio: 0.000, Total Score: 0.003\n",
      "  Pruning Ratio 0.05, Weighted Diff-Loss: 0.0029910269192429673, Weighted Ratio: 0.005, Total Score: 0.008\n",
      "  Pruning Ratio 0.1, Weighted Diff-Loss: -0.0033233632436027146, Weighted Ratio: 0.010, Total Score: 0.007\n",
      "  Pruning Ratio 0.15, Weighted Diff-Loss: -0.005815885676304396, Weighted Ratio: 0.015, Total Score: 0.009\n",
      "  Pruning Ratio 0.2, Weighted Diff-Loss: -0.02201728148886698, Weighted Ratio: 0.020, Total Score: -0.002\n",
      "  Pruning Ratio 0.25, Weighted Diff-Loss: -0.025589896975739143, Weighted Ratio: 0.025, Total Score: -0.001\n",
      "  Pruning Ratio 0.3, Weighted Diff-Loss: -0.025756065137919257, Weighted Ratio: 0.030, Total Score: 0.004\n",
      "  Pruning Ratio 0.35, Weighted Diff-Loss: -0.024094383516118627, Weighted Ratio: 0.035, Total Score: 0.011\n",
      "  Pruning Ratio 0.4, Weighted Diff-Loss: -0.06397474243934834, Weighted Ratio: 0.040, Total Score: -0.024\n",
      "  Pruning Ratio 0.45, Weighted Diff-Loss: -0.06571950814224024, Weighted Ratio: 0.045, Total Score: -0.021\n",
      "  Pruning Ratio 0.5, Weighted Diff-Loss: -0.07535726154868676, Weighted Ratio: 0.050, Total Score: -0.025\n",
      "  Pruning Ratio 0.55, Weighted Diff-Loss: -0.1261216350947159, Weighted Ratio: 0.055, Total Score: -0.071\n",
      "  Pruning Ratio 0.6, Weighted Diff-Loss: -0.23737121967430969, Weighted Ratio: 0.060, Total Score: -0.177\n",
      "  Pruning Ratio 0.65, Weighted Diff-Loss: -0.32053838484546404, Weighted Ratio: 0.065, Total Score: -0.256\n",
      "  Pruning Ratio 0.7, Weighted Diff-Loss: -0.3938185443668988, Weighted Ratio: 0.070, Total Score: -0.324\n",
      "  Pruning Ratio 0.75, Weighted Diff-Loss: -0.4117647058823527, Weighted Ratio: 0.075, Total Score: -0.337\n",
      "Best Pruning Ratio: 0.35, Highest Score: 0.011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_results = display_scores(models_data)\n",
    "#final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
