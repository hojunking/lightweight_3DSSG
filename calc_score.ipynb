{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_loss(baseline_acc, current_acc):\n",
    "    \"\"\"Calculate normalized performance loss.\"\"\"\n",
    "    #print('check: baseline_acc: ', baseline_acc, 'current_acc: ', current_acc)\n",
    "    return (current_acc - baseline_acc) / baseline_acc if baseline_acc != 0 else 0\n",
    "\n",
    "def calculate_differential_losses(pruning_results, baseline):\n",
    "    \"\"\"\n",
    "    Calculate the differential of performance losses across pruning ratios.\n",
    "    Arguments:\n",
    "    pruning_results -- list of tuples, where each tuple contains (pruning_ratio, obj_acc, rel_acc, triplet_acc)\n",
    "    baseline -- tuple of (baseline_obj_acc, baseline_rel_acc, baseline_triplet_acc)\n",
    "    \"\"\"\n",
    "    # Sorting by pruning ratios for accurate differential calculation\n",
    "    sorted_results = sorted(pruning_results, key=lambda x: x[0])\n",
    "    ratios = [x[0] for x in sorted_results]\n",
    "    losses = [[calculate_performance_loss(baseline[i], x[i + 1]) for i in range(3)] for x in sorted_results]\n",
    "    \n",
    "    # Calculating differentials of losses\n",
    "    diffs = -np.diff(np.array(losses), axis=0)\n",
    "    diffs = np.vstack([diffs, diffs[-1]])  # Duplicate last differential as an approximation for the last point\n",
    "    return ratios, losses, diffs\n",
    "\n",
    "def calculate_model_score(losses, diffs, weights=(1, 1, 5), mu=5, pruning_ratio=0, detailed=False):\n",
    "    \"\"\"Calculate scores incorporating the absolute values of losses and their differentials, adjusted by lambda.\"\"\"\n",
    "    adjusted_losses = losses\n",
    "    adjusted_diffs = diffs\n",
    "    #print(f'pruning ratio: {pruning_ratio}, adjusted losses: {adjusted_losses}, adjusted diffs: {adjusted_diffs}')\n",
    "    weighted_diff_loss = sum(weights[i] * (adjusted_diffs[i] + adjusted_losses[i]) for i in range(3))\n",
    "    score = weighted_diff_loss + mu * pruning_ratio\n",
    "    \n",
    "    if detailed:\n",
    "        return score, weighted_diff_loss\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGFN Pruning Results: [(0.0, 52.66, 89.97, 88.53), (0.05, 52.48, 90.05, 88.43), (0.1, 52.48, 89.87, 88.25), (0.15, 52.67, 89.36, 88.11), (0.2, 51.99, 88.79, 87.92), (0.25, 51.13, 87.03, 87.26), (0.3, 50.52, 85.58, 86.6), (0.35, 47.36, 85.73, 84.57), (0.4, 41.54, 81.21, 80.28), (0.45, 36.97, 73.04, 77.1), (0.5, 32.31, 24.39, 74.83), (0.55, 25.02, 19.25, 73.46), (0.6, 16.86, 17.5, 73.12), (0.65, 14.42, 15.7, 72.69), (0.7, 13.76, 12.29, 72.44), (0.75, 14.94, 6.87, 72.36), (0.8, 10.07, 5.58, 72.29), (0.85, 4.07, 5.57, 72.34), (0.9, 4.07, 5.57, 72.34), (0.95, 4.07, 5.57, 72.34)]\n",
      "SGFN Baseline: (52.66, 89.97, 88.53)\n",
      "Attention SGFN Pruning Results: [(0.0, 54.77, 88.45, 88.7), (0.05, 54.63, 88.02, 89.14), (0.1, 54.84, 86.54, 89.06), (0.15, 53.93, 84.01, 88.82), (0.2, 53.04, 81.12, 88.34), (0.25, 50.79, 76.34, 87.59), (0.3, 49.02, 71.05, 86.64), (0.35, 47.61, 65.29, 86.22), (0.4, 44.13, 60.21, 84.93), (0.45, 42.47, 52.87, 83.67), (0.5, 41.45, 54.81, 81.9), (0.55, 37.37, 52.88, 80.12), (0.6, 32.29, 57.32, 77.83), (0.65, 28.45, 59.64, 76.23), (0.7, 24.49, 56.66, 75.56), (0.75, 16.9, 63.38, 74.66), (0.8, 17.09, 69.71, 74.78), (0.85, 2.93, 72.97, 72.2), (0.9, 2.93, 72.97, 72.2), (0.95, 2.93, 72.97, 72.2)]\n",
      "Attention SGFN Baseline: (54.77, 88.45, 88.7)\n",
      "SGPN Pruning Results: [(0.0, 51.02, 82.73, 86.95), (0.05, 35.321, 78.464, 80.474), (0.1, 27.65, 78.115, 77.872), (0.15, 23.119, 77.926, 75.569), (0.2, 28.219, 75.485, 76.013), (0.25, 24.974, 77.765, 76.29), (0.3, 17.977, 78.734, 75.433), (0.35, 18.061, 77.034, 75.609), (0.4, 19.663, 78.811, 76.801), (0.45, 19.262, 79.322, 76.947), (0.5, 18.462, 79.622, 76.853), (0.55, 18.714, 79.022, 76.538), (0.6, 19.578, 78.605, 76.015), (0.65, 27.018, 77.993, 76.052), (0.7, 27.439, 77.522, 76.357), (0.75, 27.945, 77.195, 76.959), (0.8, 27.987, 76.63, 77.351), (0.85, 28.303, 78.333, 76.816), (0.9, 29.652, 77.839, 76.474), (0.95, 29.884, 77.222, 74.924)]\n",
      "SGPN Baseline: (51.02, 82.73, 86.95)\n",
      "VLSAT Pruning Results: [(0.0, 55.41, 91.18, 89.7), (0.05, 55.43, 87.31, 89.33), (0.1, 55.49, 84.51, 89.19), (0.15, 55.38, 80.08, 89.07), (0.2, 55.38, 75.19, 88.87), (0.25, 55.38, 66.55, 88.71), (0.3, 55.26, 65.47, 88.6), (0.35, 55.22, 66.15, 88.39), (0.4, 55.26, 68.81, 88.07), (0.45, 55.22, 56.65, 87.96), (0.5, 54.98, 49.83, 87.23), (0.55, 53.21, 38.84, 85.17), (0.6, 49.9, 38.84, 84.09), (0.65, 45.92, 38.84, 81.79), (0.7, 41.58, 38.84, 79.28), (0.75, 37.45, 5.83, 77.62)]\n",
      "VLSAT Baseline: (55.41, 91.18, 89.7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_dir = '/home/song/Desktop/song/lightweight_3DSSG/visualization/pruning_infer_data/'\n",
    "# CSV 파일 경로\n",
    "file_path1 = base_dir + 'sgfn_baseline_st_results.csv'\n",
    "file_path2 = base_dir +'attn_sgfn_baseline_st_results.csv'\n",
    "file_path3 = base_dir +'sgpn_baseline_st_results.csv'\n",
    "file_paht4 = base_dir + 'VLSAT_param65_Structured_pruninig_results.csv'\n",
    "# CSV 파일 로드\n",
    "df1 = pd.read_csv(file_path1).sort_values(by='Pruning Ratio')\n",
    "df2 = pd.read_csv(file_path2).sort_values(by='Pruning Ratio')\n",
    "df3 = pd.read_csv(file_path3).sort_values(by='Pruning Ratio')\n",
    "df4 = pd.read_csv(file_paht4).sort_values(by='Pruning Ratio')\n",
    "\n",
    "\n",
    "# 필요한 데이터를 추출하여 리스트로 변환\n",
    "sgfn_pruning_results = [\n",
    "    (row['Pruning Ratio'], row['3d obj Acc@1'], row['3d rel Acc@1'], row['3d triplet Acc@50'])\n",
    "    for index, row in df1.iterrows()\n",
    "]\n",
    "sgfn_baseline = (\n",
    "    df1['3d obj Acc@1'].iloc[0],\n",
    "    df1['3d rel Acc@1'].iloc[0],\n",
    "    df1['3d triplet Acc@50'].iloc[0]\n",
    ")\n",
    "attn_sgfn_pruning_results = [\n",
    "    (row['Pruning Ratio'], row['3d obj Acc@1'], row['3d rel Acc@1'], row['3d triplet Acc@50'])\n",
    "    for index, row in df2.iterrows()\n",
    "]\n",
    "attn_sgfn_baseline = (\n",
    "    df2['3d obj Acc@1'].iloc[0],\n",
    "    df2['3d rel Acc@1'].iloc[0],\n",
    "    df2['3d triplet Acc@50'].iloc[0]\n",
    ")\n",
    "\n",
    "sgpn_pruning_results = [\n",
    "    (row['Pruning Ratio'], row['3d obj Acc@1'], row['3d rel Acc@1'], row['3d triplet Acc@50'])\n",
    "    for index, row in df3.iterrows()\n",
    "]\n",
    "sgpn_baseline = (\n",
    "    df3['3d obj Acc@1'].iloc[0],\n",
    "    df3['3d rel Acc@1'].iloc[0],\n",
    "    df3['3d triplet Acc@50'].iloc[0]\n",
    ")\n",
    "\n",
    "vlsat_pruning_results = [\n",
    "    (row['Pruning Ratio'], row['3d obj Acc@1'], row['3d rel Acc@1'], row['3d triplet Acc@50'])\n",
    "    for index, row in df4.iterrows()\n",
    "]\n",
    "vlsat_baseline = (\n",
    "    df4['3d obj Acc@1'].iloc[0],\n",
    "    df4['3d rel Acc@1'].iloc[0],\n",
    "    df4['3d triplet Acc@50'].iloc[0]\n",
    ")\n",
    "print(\"SGFN Pruning Results:\", sgfn_pruning_results)\n",
    "print(\"SGFN Baseline:\", sgfn_baseline)\n",
    "print(\"Attention SGFN Pruning Results:\", attn_sgfn_pruning_results)\n",
    "print(\"Attention SGFN Baseline:\", attn_sgfn_baseline)\n",
    "print(\"SGPN Pruning Results:\", sgpn_pruning_results)\n",
    "print(\"SGPN Baseline:\", sgpn_baseline)\n",
    "print(\"VLSAT Pruning Results:\", vlsat_pruning_results)\n",
    "print(\"VLSAT Baseline:\", vlsat_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_data = {\n",
    "    \"SGPN\": {\n",
    "        \"baseline\": sgpn_baseline,\n",
    "        \"pruning_results\": sgpn_pruning_results\n",
    "    },\n",
    "    \"SGFN\": {\n",
    "        \"baseline\": sgfn_baseline,\n",
    "        \"pruning_results\": sgfn_pruning_results\n",
    "    },\n",
    "    \"Attn + SGFN\": {\n",
    "        \"baseline\": attn_sgfn_baseline,\n",
    "        \"pruning_results\": attn_sgfn_pruning_results\n",
    "    },\n",
    "    \"VLSAT\": {\n",
    "        \"baseline\": vlsat_baseline,\n",
    "        \"pruning_results\": vlsat_pruning_results\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(models_data, weights=(0, 0, 5), mu=0.5):\n",
    "    results = {}\n",
    "    for model_name, data in models_data.items():\n",
    "        baseline = data[\"baseline\"]\n",
    "        pruning_results = data[\"pruning_results\"]\n",
    "        if not pruning_results or len(pruning_results) < 2:\n",
    "            print(f\"Not enough data to calculate scores for {model_name}.\")\n",
    "            continue\n",
    "        \n",
    "        ratios, losses, diffs = calculate_differential_losses(pruning_results, baseline)\n",
    "        \n",
    "        # Store detailed score components for each pruning ratio\n",
    "        detailed_scores = []\n",
    "        scores = []\n",
    "        for idx in range(len(ratios)):\n",
    "            score, components = calculate_model_score(losses[idx], diffs[idx], weights, mu, ratios[idx], detailed=True)\n",
    "            scores.append(score)\n",
    "            detailed_scores.append({\n",
    "                \"ratio\": ratios[idx],\n",
    "                \"weighted_diff_loss\": components,\n",
    "                \"weighted_ratio\": mu * ratios[idx],\n",
    "                \"total_score\": score\n",
    "            })\n",
    "        \n",
    "        max_score_index = np.argmax(scores)\n",
    "        best_ratio = ratios[max_score_index]\n",
    "        best_score = scores[max_score_index]\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"best_ratio\": best_ratio,\n",
    "            \"best_score\": best_score,\n",
    "            \"all_scores\": scores,\n",
    "            \"detailed_scores\": detailed_scores\n",
    "        }\n",
    "\n",
    "        print(f\"Scores for {model_name}:\")\n",
    "        for entry in detailed_scores:\n",
    "            print(f\"  Pruning Ratio {entry['ratio']}, Weighted Diff-Loss: {entry['weighted_diff_loss']}, \"\n",
    "                  f\"Weighted Ratio: {entry['weighted_ratio']:.3f}, Total Score: {entry['total_score']:.3f}\")\n",
    "        print(f\"Best Pruning Ratio: {best_ratio}, Highest Score: {best_score:.3f}\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for SGPN:\n",
      "  Pruning Ratio 0.0, Weighted Diff-Loss: 0.3723979298447383, Weighted Ratio: 0.000, Total Score: 0.372\n",
      "  Pruning Ratio 0.05, Weighted Diff-Loss: -0.22277170787809059, Weighted Ratio: 0.025, Total Score: -0.198\n",
      "  Pruning Ratio 0.1, Weighted Diff-Loss: -0.38959171937895376, Weighted Ratio: 0.050, Total Score: -0.340\n",
      "  Pruning Ratio 0.15, Weighted Diff-Loss: -0.6799884991374354, Weighted Ratio: 0.075, Total Score: -0.605\n",
      "  Pruning Ratio 0.2, Weighted Diff-Loss: -0.6448533640023001, Weighted Ratio: 0.100, Total Score: -0.545\n",
      "  Pruning Ratio 0.25, Weighted Diff-Loss: -0.5637147786083956, Weighted Ratio: 0.125, Total Score: -0.439\n",
      "  Pruning Ratio 0.3, Weighted Diff-Loss: -0.6723979298447373, Weighted Ratio: 0.150, Total Score: -0.522\n",
      "  Pruning Ratio 0.35, Weighted Diff-Loss: -0.7207015526164471, Weighted Ratio: 0.175, Total Score: -0.546\n",
      "  Pruning Ratio 0.4, Weighted Diff-Loss: -0.5920069005175389, Weighted Ratio: 0.200, Total Score: -0.392\n",
      "  Pruning Ratio 0.45, Weighted Diff-Loss: -0.5698102357676822, Weighted Ratio: 0.225, Total Score: -0.345\n",
      "  Pruning Ratio 0.5, Weighted Diff-Loss: -0.5625071880391035, Weighted Ratio: 0.250, Total Score: -0.313\n",
      "  Pruning Ratio 0.55, Weighted Diff-Loss: -0.568660149511214, Weighted Ratio: 0.275, Total Score: -0.294\n",
      "  Pruning Ratio 0.6, Weighted Diff-Loss: -0.6309373202990227, Weighted Ratio: 0.300, Total Score: -0.331\n",
      "  Pruning Ratio 0.65, Weighted Diff-Loss: -0.6442208165612415, Weighted Ratio: 0.325, Total Score: -0.319\n",
      "  Pruning Ratio 0.7, Weighted Diff-Loss: -0.6437607820586548, Weighted Ratio: 0.350, Total Score: -0.294\n",
      "  Pruning Ratio 0.75, Weighted Diff-Loss: -0.5970672800460032, Weighted Ratio: 0.375, Total Score: -0.222\n",
      "  Pruning Ratio 0.8, Weighted Diff-Loss: -0.5212190914318579, Weighted Ratio: 0.400, Total Score: -0.121\n",
      "  Pruning Ratio 0.85, Weighted Diff-Loss: -0.5630822311673376, Weighted Ratio: 0.425, Total Score: -0.138\n",
      "  Pruning Ratio 0.9, Weighted Diff-Loss: -0.5132834962622199, Weighted Ratio: 0.450, Total Score: -0.063\n",
      "  Pruning Ratio 0.95, Weighted Diff-Loss: -0.6024151811385854, Weighted Ratio: 0.475, Total Score: -0.127\n",
      "Best Pruning Ratio: 0.0, Highest Score: 0.372\n",
      "\n",
      "Scores for SGFN:\n",
      "  Pruning Ratio 0.0, Weighted Diff-Loss: 0.005647803004630877, Weighted Ratio: 0.000, Total Score: 0.006\n",
      "  Pruning Ratio 0.05, Weighted Diff-Loss: 0.004518242403705665, Weighted Ratio: 0.025, Total Score: 0.030\n",
      "  Pruning Ratio 0.1, Weighted Diff-Loss: -0.007906924206483711, Weighted Ratio: 0.050, Total Score: 0.042\n",
      "  Pruning Ratio 0.15, Weighted Diff-Loss: -0.01298994691065198, Weighted Ratio: 0.075, Total Score: 0.062\n",
      "  Pruning Ratio 0.2, Weighted Diff-Loss: 0.0028239015023154377, Weighted Ratio: 0.100, Total Score: 0.103\n",
      "  Pruning Ratio 0.25, Weighted Diff-Loss: -0.034451598328249476, Weighted Ratio: 0.125, Total Score: 0.091\n",
      "  Pruning Ratio 0.3, Weighted Diff-Loss: 0.005647803004630893, Weighted Ratio: 0.150, Total Score: 0.156\n",
      "  Pruning Ratio 0.35, Weighted Diff-Loss: 0.018637749915282054, Weighted Ratio: 0.175, Total Score: 0.194\n",
      "  Pruning Ratio 0.4, Weighted Diff-Loss: -0.2863436123348015, Weighted Ratio: 0.200, Total Score: -0.086\n",
      "  Pruning Ratio 0.45, Weighted Diff-Loss: -0.5173387552242182, Weighted Ratio: 0.225, Total Score: -0.292\n",
      "  Pruning Ratio 0.5, Weighted Diff-Loss: -0.6963741104710266, Weighted Ratio: 0.250, Total Score: -0.446\n",
      "  Pruning Ratio 0.55, Weighted Diff-Loss: -0.8319213825821765, Weighted Ratio: 0.275, Total Score: -0.557\n",
      "  Pruning Ratio 0.6, Weighted Diff-Loss: -0.8460408900937528, Weighted Ratio: 0.300, Total Score: -0.546\n",
      "  Pruning Ratio 0.65, Weighted Diff-Loss: -0.880492488422004, Weighted Ratio: 0.325, Total Score: -0.555\n",
      "  Pruning Ratio 0.7, Weighted Diff-Loss: -0.904213261041455, Weighted Ratio: 0.350, Total Score: -0.554\n",
      "  Pruning Ratio 0.75, Weighted Diff-Loss: -0.9092962837456235, Weighted Ratio: 0.375, Total Score: -0.534\n",
      "  Pruning Ratio 0.8, Weighted Diff-Loss: -0.9200271094544217, Weighted Ratio: 0.400, Total Score: -0.520\n",
      "  Pruning Ratio 0.85, Weighted Diff-Loss: -0.9143793064497908, Weighted Ratio: 0.425, Total Score: -0.489\n",
      "  Pruning Ratio 0.9, Weighted Diff-Loss: -0.9143793064497908, Weighted Ratio: 0.450, Total Score: -0.464\n",
      "  Pruning Ratio 0.95, Weighted Diff-Loss: -0.9143793064497908, Weighted Ratio: 0.475, Total Score: -0.439\n",
      "Best Pruning Ratio: 0.35, Highest Score: 0.194\n",
      "\n",
      "Scores for Attn + SGFN:\n",
      "  Pruning Ratio 0.0, Weighted Diff-Loss: -0.02480270574971802, Weighted Ratio: 0.000, Total Score: -0.025\n",
      "  Pruning Ratio 0.05, Weighted Diff-Loss: 0.02931228861330304, Weighted Ratio: 0.025, Total Score: 0.054\n",
      "  Pruning Ratio 0.1, Weighted Diff-Loss: 0.033821871476888865, Weighted Ratio: 0.050, Total Score: 0.084\n",
      "  Pruning Ratio 0.15, Weighted Diff-Loss: 0.03382187147688727, Weighted Ratio: 0.075, Total Score: 0.109\n",
      "  Pruning Ratio 0.2, Weighted Diff-Loss: 0.02198421645997748, Weighted Ratio: 0.100, Total Score: 0.122\n",
      "  Pruning Ratio 0.25, Weighted Diff-Loss: -0.009019165727170049, Weighted Ratio: 0.125, Total Score: 0.116\n",
      "  Pruning Ratio 0.3, Weighted Diff-Loss: -0.09244644870349496, Weighted Ratio: 0.150, Total Score: 0.058\n",
      "  Pruning Ratio 0.35, Weighted Diff-Loss: -0.06708004509582928, Weighted Ratio: 0.175, Total Score: 0.108\n",
      "  Pruning Ratio 0.4, Weighted Diff-Loss: -0.1414881623449826, Weighted Ratio: 0.200, Total Score: 0.059\n",
      "  Pruning Ratio 0.45, Weighted Diff-Loss: -0.18376550169109385, Weighted Ratio: 0.225, Total Score: 0.041\n",
      "  Pruning Ratio 0.5, Weighted Diff-Loss: -0.2829763246899659, Weighted Ratio: 0.250, Total Score: -0.033\n",
      "  Pruning Ratio 0.55, Weighted Diff-Loss: -0.3545659526493795, Weighted Ratio: 0.275, Total Score: -0.080\n",
      "  Pruning Ratio 0.6, Weighted Diff-Loss: -0.5225479143179261, Weighted Ratio: 0.300, Total Score: -0.223\n",
      "  Pruning Ratio 0.65, Weighted Diff-Loss: -0.6651634723788048, Weighted Ratio: 0.325, Total Score: -0.340\n",
      "  Pruning Ratio 0.7, Weighted Diff-Loss: -0.6899661781285228, Weighted Ratio: 0.350, Total Score: -0.340\n",
      "  Pruning Ratio 0.75, Weighted Diff-Loss: -0.7981961668545666, Weighted Ratio: 0.375, Total Score: -0.423\n",
      "  Pruning Ratio 0.8, Weighted Diff-Loss: -0.6392333709131908, Weighted Ratio: 0.400, Total Score: -0.239\n",
      "  Pruning Ratio 0.85, Weighted Diff-Loss: -0.9301014656144306, Weighted Ratio: 0.425, Total Score: -0.505\n",
      "  Pruning Ratio 0.9, Weighted Diff-Loss: -0.9301014656144306, Weighted Ratio: 0.450, Total Score: -0.480\n",
      "  Pruning Ratio 0.95, Weighted Diff-Loss: -0.9301014656144306, Weighted Ratio: 0.475, Total Score: -0.455\n",
      "Best Pruning Ratio: 0.2, Highest Score: 0.122\n",
      "\n",
      "Scores for VLSAT:\n",
      "  Pruning Ratio 0.0, Weighted Diff-Loss: 0.029480476137501453, Weighted Ratio: 0.000, Total Score: 0.029\n",
      "  Pruning Ratio 0.05, Weighted Diff-Loss: -0.03337412392924733, Weighted Ratio: 0.025, Total Score: -0.008\n",
      "  Pruning Ratio 0.1, Weighted Diff-Loss: -0.02391812214929317, Weighted Ratio: 0.050, Total Score: 0.026\n",
      "  Pruning Ratio 0.15, Weighted Diff-Loss: -0.02224941595283076, Weighted Ratio: 0.075, Total Score: 0.053\n",
      "  Pruning Ratio 0.2, Weighted Diff-Loss: -0.026143063744576645, Weighted Ratio: 0.100, Total Score: 0.074\n",
      "  Pruning Ratio 0.25, Weighted Diff-Loss: -0.03114918233396307, Weighted Ratio: 0.125, Total Score: 0.094\n",
      "  Pruning Ratio 0.3, Weighted Diff-Loss: -0.03170541773278492, Weighted Ratio: 0.150, Total Score: 0.118\n",
      "  Pruning Ratio 0.35, Weighted Diff-Loss: -0.043942596506842825, Weighted Ratio: 0.175, Total Score: 0.131\n",
      "  Pruning Ratio 0.4, Weighted Diff-Loss: -0.04171765491155778, Weighted Ratio: 0.200, Total Score: 0.158\n",
      "  Pruning Ratio 0.45, Weighted Diff-Loss: -0.03615530092335107, Weighted Ratio: 0.225, Total Score: 0.189\n",
      "  Pruning Ratio 0.5, Weighted Diff-Loss: -0.03671153632217136, Weighted Ratio: 0.250, Total Score: 0.213\n",
      "  Pruning Ratio 0.55, Weighted Diff-Loss: -0.16742685504505542, Weighted Ratio: 0.275, Total Score: 0.108\n",
      "  Pruning Ratio 0.6, Weighted Diff-Loss: -0.40549560574034965, Weighted Ratio: 0.300, Total Score: -0.105\n",
      "  Pruning Ratio 0.65, Weighted Diff-Loss: -0.4639003226165307, Weighted Ratio: 0.325, Total Score: -0.139\n",
      "  Pruning Ratio 0.7, Weighted Diff-Loss: -0.5067304483257322, Weighted Ratio: 0.350, Total Score: -0.157\n",
      "  Pruning Ratio 0.75, Weighted Diff-Loss: -0.6841695405495603, Weighted Ratio: 0.375, Total Score: -0.309\n",
      "  Pruning Ratio 0.8, Weighted Diff-Loss: -0.8666147513627767, Weighted Ratio: 0.400, Total Score: -0.467\n",
      "  Pruning Ratio 0.85, Weighted Diff-Loss: -0.9711870063410832, Weighted Ratio: 0.425, Total Score: -0.546\n",
      "  Pruning Ratio 0.9, Weighted Diff-Loss: -0.9711870063410832, Weighted Ratio: 0.450, Total Score: -0.521\n",
      "  Pruning Ratio 0.95, Weighted Diff-Loss: -0.9711870063410832, Weighted Ratio: 0.475, Total Score: -0.496\n",
      "Best Pruning Ratio: 0.5, Highest Score: 0.213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_results = display_scores(models_data)\n",
    "#final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일이 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# SGFN 구조화된 및 비구조화된 프루닝 결과\n",
    "vlsat_structured_data = {\n",
    "    \"Pruning Ratio\": [0.7, 0.55, 0.3, 0.25, 0.2, 0, 0.35, 0.45, 0.05, 0.65, 0.4, 0.1, 0.6, 0.15, 0.5, 0.75],\n",
    "    \"3d obj Acc@1\": [41.581, 53.214, 55.258, 55.385, 55.385, 55.406, 55.216, 55.216, 55.427, 45.922, 55.258, 55.490, 49.905, 55.385, 54.984, 37.450],\n",
    "    \"3d rel Acc@1\": [38.845, 38.845, 65.470, 66.546, 75.195, 91.175, 66.152, 56.646, 87.311, 38.845, 68.807, 84.512, 38.845, 80.078, 49.831, 5.830],\n",
    "    \"3d triplet Acc@50\": [79.280, 85.169, 88.605, 88.712, 88.870, 89.701, 88.387, 87.965, 89.329, 81.791, 88.070, 89.187, 84.094, 89.068, 87.227, 77.621]\n",
    "}\n",
    "\n",
    "unstructured_data = {\n",
    "    \"Pruning Ratio\": [0.8, 0.65, 0.85, 0.75, 0.45, 0.5, 0.35, 0.3, 0.25, 0.6, 0.05, 0.7, 0.1, 0.4, 0.9, 0.15, 0.95, 0.2, 0.55],\n",
    "    \"3d obj Acc@1\": [38.609, 49.631, 32.455, 43.267, 51.949, 51.823, 52.055, 52.308, 52.624, 50.896, 52.476, 46.575, 52.476, 51.970, 23.878, 52.497, 4.067, 52.518, 51.149],\n",
    "    \"3d rel Acc@1\": [85.145, 89.071, 79.773, 87.398, 89.807, 89.681, 89.879, 89.998, 89.986, 89.267, 89.956, 88.337, 89.946, 89.874, 53.827, 89.973, 13.326, 89.906, 89.525],\n",
    "    \"3d triplet Acc@50\": [82.103, 87.256, 77.862, 84.398, 88.417, 88.208, 88.528, 88.506, 88.491, 87.631, 88.508, 85.920, 88.516, 88.508, 73.616, 88.508, 72.639, 88.513, 87.928]\n",
    "}\n",
    "# DataFrame 생성 및 정렬\n",
    "df_structured = pd.DataFrame(vlsat_structured_data).sort_values('Pruning Ratio').round(2)\n",
    "#df_unstructured = pd.DataFrame(unstructured_data).sort_values('Pruning Ratio').round(2)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df_structured.to_csv('VLSAT_param65_Structured_pruninig_results.csv', index=False)\n",
    "#df_unstructured.to_csv('sgfn_baseline_unst_results.csv', index=False)\n",
    "\n",
    "print(\"CSV 파일이 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
